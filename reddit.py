import requests
import logging
import os
from random import choice
from queue_manager import (
    initialize_queue_db, 
    add_json_batch, 
    get_next_unposted_post, 
    get_queue_stats
)

logger = logging.getLogger(__name__)

# Configura√ß√µes do CCProxy (seu PC)
PROXY_HOST = os.getenv("PROXY_HOST", "")
PROXY_PORT = os.getenv("PROXY_PORT", "8080")
PROXY_USER = os.getenv("PROXY_USER", "")
PROXY_PASS = os.getenv("PROXY_PASS", "")

def check_proxy_available():
    """
    Verifica se o CCProxy est√° online e acess√≠vel.
    Retorna True se dispon√≠vel, False caso contr√°rio.
    """
    if not PROXY_HOST:
        logger.warning("‚ö†Ô∏è PROXY_HOST n√£o configurado")
        return False
    
    try:
        # Monta URL do proxy com autentica√ß√£o
        if PROXY_USER and PROXY_PASS:
            proxy_url = f"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}:{PROXY_PORT}"
        else:
            proxy_url = f"http://{PROXY_HOST}:{PROXY_PORT}"
        
        proxies = {
            "http": proxy_url,
            "https": proxy_url
        }
        
        logger.info(f"üîç Verificando CCProxy: {PROXY_HOST}:{PROXY_PORT}")
        
        # Tenta requisi√ß√£o simples
        response = requests.get(
            "https://www.reddit.com/r/test.json?limit=1",
            proxies=proxies,
            timeout=5
        )
        
        if response.status_code == 200:
            logger.info("‚úÖ CCProxy DISPON√çVEL!")
            return True
        else:
            logger.warning(f"‚ö†Ô∏è CCProxy respondeu com status {response.status_code}")
            return False
            
    except requests.exceptions.Timeout:
        logger.warning("‚è±Ô∏è CCProxy: Timeout (PC pode estar desligado)")
        return False
    except requests.exceptions.ProxyError:
        logger.warning("üîå CCProxy: Erro de proxy (verifique usu√°rio/senha)")
        return False
    except requests.exceptions.ConnectionError:
        logger.warning("üîå CCProxy: Conex√£o recusada (PC desligado ou CCProxy n√£o rodando)")
        return False
    except Exception as e:
        logger.warning(f"‚ö†Ô∏è Erro ao verificar CCProxy: {e}")
        return False

def fetch_posts_from_reddit(subreddit, limit=50):
    """
    Busca posts do Reddit usando CCProxy.
    Retorna lista de posts ou None se falhar.
    """
    # Monta URL do proxy com autentica√ß√£o
    if PROXY_USER and PROXY_PASS:
        proxy_url = f"http://{PROXY_USER}:{PROXY_PASS}@{PROXY_HOST}:{PROXY_PORT}"
    else:
        proxy_url = f"http://{PROXY_HOST}:{PROXY_PORT}"
    
    proxies = {
        "http": proxy_url,
        "https": proxy_url
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'en-US,en;q=0.9',
    }
    
    # URLs para tentar
    urls = [
        f"https://www.reddit.com/r/{subreddit}/hot.json?limit={limit}",
        f"https://www.reddit.com/r/{subreddit}.json?limit={limit}",
        f"https://old.reddit.com/r/{subreddit}/hot.json?limit={limit}",
    ]
    
    for url_index, url in enumerate(urls, 1):
        try:
            logger.info(f"üåê Tentativa {url_index}/{len(urls)}: {url}")
            
            response = requests.get(
                url,
                headers=headers,
                proxies=proxies,
                timeout=20
            )
            response.raise_for_status()
            
            data = response.json()
            posts = []
            
            # Processa TODOS os posts do JSON
            for post in data['data']['children']:
                post_data = post['data']
                
                # Pula posts fixados
                if post_data.get('stickied', False):
                    continue
                
                post_info = {
                    "id": post_data['id'],
                    "title": post_data.get('title', ''),
                    "content": post_data.get('selftext', ''),
                    "url": f"https://www.reddit.com{post_data.get('permalink', '')}",
                    "s_img": '',
                    "m_img": [],
                    "video": '',
                    "video_fallback_url": ''
                }
                
                # Imagem √∫nica
                if 'preview' in post_data and 'images' in post_data['preview']:
                    try:
                        image_url = post_data['preview']['images'][0]['source']['url']
                        post_info["s_img"] = image_url.replace('&amp;', '&')
                    except:
                        pass
                
                # Galeria
                if post_data.get('is_gallery') and 'gallery_data' in post_data:
                    try:
                        images = []
                        for item in post_data['gallery_data']['items'][:4]:
                            media_id = item['media_id']
                            if media_id in post_data.get('media_metadata', {}):
                                if 's' in post_data['media_metadata'][media_id]:
                                    if 'u' in post_data['media_metadata'][media_id]['s']:
                                        img_url = post_data['media_metadata'][media_id]['s']['u']
                                        images.append(img_url.replace('&amp;', '&'))
                        post_info["m_img"] = images
                    except Exception as e:
                        logger.debug(f"Erro ao extrair galeria: {e}")
                
                # V√≠deo
                if post_data.get('is_video') and 'media' in post_data:
                    if post_data['media'] and 'reddit_video' in post_data['media']:
                        post_info["video"] = f"https://www.reddit.com{post_data['permalink']}"
                        post_info["video_fallback_url"] = post_data['media']['reddit_video'].get('fallback_url', '')
                
                posts.append(post_info)
            
            logger.info(f"‚úÖ {len(posts)} posts obtidos de r/{subreddit}!")
            return posts
            
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 402:
                logger.warning(f"‚ùå Erro 402 na tentativa {url_index} - proxy bloqueou endpoint")
                continue
            else:
                logger.warning(f"‚ùå HTTP {e.response.status_code} na tentativa {url_index}")
                continue
        except Exception as e:
            logger.warning(f"‚ùå Falha na tentativa {url_index}: {e}")
            continue
    
    logger.error("‚ùå Todas as tentativas falharam via CCProxy")
    return None

def extractContent():
    """
    Sistema otimizado de extra√ß√£o:
    
    1. Verifica se CCProxy est√° dispon√≠vel
    2. Se SIM: busca 50 posts e adiciona como 1 JSON batch (m√°x 2 no DB)
    3. Se N√ÉO: busca da fila existente
    4. Varre JSON inteiro procurando post n√£o visto
    5. Se JSON esgota, remove e passa para pr√≥ximo
    6. Retorna sempre 1 post novo (ou lista vazia se n√£o houver)
    """
    # Inicializa DB
    initialize_queue_db()
    
    # Mostra estat√≠sticas
    stats = get_queue_stats()
    logger.info("=" * 60)
    logger.info(f"üìä Status da Fila:")
    logger.info(f"   Batches armazenados: {stats['batches_count']}/{MAX_JSON_BATCHES}")
    logger.info(f"   Posts dispon√≠veis: {stats['available_posts']}")
    logger.info(f"   Total postados: {stats['posted_total']}")
    
    if stats['batches']:
        for batch in stats['batches']:
            logger.info(f"   ‚Ä¢ Batch #{batch['batch_id']}: r/{batch['subreddit']} - {batch['remaining']}/{batch['total']} restantes")
    
    logger.info("=" * 60)
    
    # Verifica se CCProxy est√° dispon√≠vel
    proxy_available = check_proxy_available()
    
    if proxy_available and stats['batches_count'] < MAX_JSON_BATCHES:
        logger.info("üü¢ MODO ONLINE: Buscando novos posts via CCProxy...")
        
        # Escolhe subreddit aleat√≥rio
        subreddits = ['Overwatch', 'Overwatch_Memes']
        subreddit = choice(subreddits)
        
        logger.info(f"üé≤ Subreddit selecionado: r/{subreddit}")
        
        # Busca posts
        posts = fetch_posts_from_reddit(subreddit, limit=50)
        
        if posts:
            # Adiciona como 1 batch (FIFO autom√°tico se j√° tiver 2)
            batch_id = add_json_batch(posts, subreddit)
            logger.info(f"üíæ Batch #{batch_id} salvo com {len(posts)} posts")
            
            # Atualiza estat√≠sticas
            stats = get_queue_stats()
            logger.info(f"üìä Fila atualizada: {stats['batches_count']} batch(es), {stats['available_posts']} posts dispon√≠veis")
        else:
            logger.warning("‚ö†Ô∏è Falha ao buscar novos posts")
    
    elif proxy_available and stats['batches_count'] >= MAX_JSON_BATCHES:
        logger.info(f"‚è∏Ô∏è J√° temos {MAX_JSON_BATCHES} batches salvos (m√°ximo), usando fila existente")
    
    elif not proxy_available:
        logger.info("üî¥ MODO OFFLINE: CCProxy indispon√≠vel, usando fila existente")
    
    # Busca pr√≥ximo post n√£o visto (varre todos os batches)
    logger.info("üîç Procurando pr√≥ximo post n√£o visto...")
    batch_id, post = get_next_unposted_post()
    
    if post:
        logger.info(f"‚ú® Post encontrado no batch #{batch_id}")
        logger.info(f"üì§ T√≠tulo: {post['title'][:70]}...")
        return [post]
    else:
        logger.error("‚ùå FILA VAZIA! Nenhum post novo dispon√≠vel.")
        logger.error("   Aguardando CCProxy ficar online para buscar mais posts...")
        return []

def debug_data(posts):
    """Fun√ß√£o de debug"""
    if posts:
        for post in posts:
            print("\nüîπ Post selecionado:")
            print(f"  ID: {post['id']}")
            print(f"  T√≠tulo: {post['title'][:70]}...")
            print(f"  URL: {post['url']}")
            if post['s_img']:
                print(f"  Imagem: Sim")
            if post['m_img']:
                print(f"  Galeria: {len(post['m_img'])} imagens")
            if post['video']:
                print(f"  V√≠deo: Sim")
        print()
    else:
        print("\n‚ùå Nenhum post dispon√≠vel.\n")

# Importa MAX_JSON_BATCHES
from queue_manager import MAX_JSON_BATCHES

if __name__ == "__main__":
    # Teste local
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    
    print("=" * 60)
    print("Sistema Otimizado de Proxy Local com CCProxy")
    print(f"M√°ximo de batches: {MAX_JSON_BATCHES}")
    print("=" * 60)
    print()
    
    posts = extractContent()
    debug_data(posts)